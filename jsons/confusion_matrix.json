{
    "question": "<p>Confusion Matrix, Precision &amp; Recall</p>",
    "answer": "<p>A confusion matrix is a table used to evaluate a classification model's performance. It summarizes the predicted and actual classifications of a model on a set of test data, showing the number of true positives, true negatives, false positives, and false negatives. It's a useful tool for understanding the strengths and weaknesses of a classifier, particularly in terms of how well it distinguishes between different classes. The total accuracy is not an good performance metric when there is an imbalance in the data set.</p>\n<ol>\n<li><strong>Precision</strong>: Precision is a metric that measures the proportion of <strong>true positive predictions among all positive predictions</strong> made by a classifier. Precision focuses on the accuracy of positive predictions.</li>\n<li><strong>Recall (Sensitivity)</strong>: Recall is a metric that measures the proportion of <strong>true positive predictions among all actual positive instances</strong> in the data. Recall focuses on how well the classifier identifies positive instances.</li>\n<li><strong>F1 Score</strong>: The F1 score is the harmonic mean of precision and recall, providing a single metric to evaluate a classifier's performance that balances both precision and recall. </li>\n</ol>\n<p>Precision and recall can be perfectly separable when the data is perfectly separable. These metrics are commonly used in evaluating binary classification models but can be extended to multi-class classification by averaging over all classes or using weighted averages.</p>\n<p><img alt=\"alt text\" src=\"../assets/images/confusionMatrxiUpdated.jpg\" /></p>",
    "prev": "bias_variance_tradeoff",
    "next": "roc_aoc"
}