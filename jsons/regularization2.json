{
    "question": "<p>Feature selection: Ridge vs Lasso</p>\n",
    "answer": "<h4>Ridge Regression: No Feature Selection</h4>\n\n<p>We start with the Ridge loss function for a single feature $x$ and coefficient $\\beta$:</p>\n\n<p>$$\nL_2 = (y - x\\beta)^2 + \\lambda \\beta^2\n$$</p>\n\n<p>Expanding the squared loss term:</p>\n\n<p>$$\nL_2 = y^2 - 2xy\\beta + x^2\\beta^2 + \\lambda \\beta^2\n$$</p>\n\n<p>Taking derivative w.r.t. $\\beta$ and setting to zero:</p>\n\n<p>$$\n\\frac{dL_2}{d\\beta} = -2xy + 2x^2\\beta + 2\\lambda\\beta = 0\n$$</p>\n\n<p>Solving:</p>\n\n<p>$$\n2(x^2 + \\lambda)\\beta = 2xy \\quad \\Rightarrow \\quad \\beta = \\frac{xy}{x^2 + \\lambda}\n$$</p>\n\n<p><strong>Key Insight:</strong>\nThe denominator $x^2 + \\lambda$ is always <strong>strictly positive</strong>, and thus $\\beta \\neq 0$ unless $xy = 0$. Even for large $\\lambda$, $\\beta$ gets <strong>shrunk</strong>, but <strong>never exactly zero</strong> unless the correlation with $y$ vanishes.</p>\n\n<hr />\n\n<h4>Lasso Regression: Can Set Coefficients to Zero</h4>\n\n<p>Now consider the Lasso loss:</p>\n\n<p>$$\nL_1 = (y - x\\beta)^2 + \\lambda |\\beta|\n$$</p>\n\n<p>Assume $\\beta &gt; 0$ for simplicity (similar logic applies for $\\beta &lt; 0$ or $\\beta = 0$):</p>\n\n<p>Expanding:</p>\n\n<p>$$\nL_1 = y^2 - 2xy\\beta + x^2\\beta^2 + \\lambda \\beta\n$$</p>\n\n<p>Taking derivative and setting to zero:</p>\n\n<p>$$\n\\frac{dL_1}{d\\beta} = -2xy + 2x^2\\beta + \\lambda = 0\n$$</p>\n\n<p>Solving:</p>\n\n<p>$$\n2x^2\\beta = 2xy - \\lambda \\quad \\Rightarrow \\quad \\beta = \\frac{2xy - \\lambda}{2x^2}\n$$</p>\n\n<p><strong>Key Insight:</strong>\nIf $2xy \\leq \\lambda$, then $\\beta \\leq 0$. Since we assumed $\\beta &gt; 0$, this implies <strong>no valid solution in the positive domain</strong>, and the optimizer sets $\\beta = 0$.</p>\n\n<p>For small correlations $xy$, a moderate $\\lambda$ is sufficient to push the entire numerator $(2xy - \\lambda)$ to zero or negative, leading to <strong>exactly zero</strong> $\\beta$. This is how <strong>feature selection</strong> occurs.</p>\n\n<hr />\n\n<h2>✅ Summary:</h2>\n\n<table>\n<thead>\n<tr>\n  <th>Aspect</th>\n  <th>Ridge</th>\n  <th>Lasso</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>Regularization</td>\n  <td>$\\lambda |\\beta|_2^2$</td>\n  <td>$\\lambda |\\beta|_1$</td>\n</tr>\n<tr>\n  <td>Penalization effect</td>\n  <td>Shrinks coefficients</td>\n  <td>Shrinks &amp; can set to <strong>exact zero</strong></td>\n</tr>\n<tr>\n  <td>Derivative behavior</td>\n  <td>Always smooth</td>\n  <td>Has a <strong>kink</strong> at zero</td>\n</tr>\n<tr>\n  <td>Geometry of penalty</td>\n  <td>Circular (smooth ball)</td>\n  <td>Diamond-shaped (corners)</td>\n</tr>\n<tr>\n  <td>Feature selection?</td>\n  <td>❌ No</td>\n  <td>✅ Yes</td>\n</tr>\n</tbody>\n</table>\n",
    "prev": "regularization",
    "next": "curse_of_dimensionality"
}