{
    "question": "<p>Gradient Descent</p>\n",
    "answer": "<p>Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent, as defined by the negative of the gradient.</p>\n\n<h3>How it Works:</h3>\n\n<ul>\n<li><strong>Initialize Parameters</strong>: Start with an initial guess for the parameters (weights and biases, for instance). These are often set randomly.</li>\n<li><strong>Compute the Gradient</strong>: The gradient is a vector of partial derivatives of the loss function with respect to each parameter. It points in the direction of the steepest increase in the loss.</li>\n<li><p><strong>Update Parameters</strong>: Update the parameters by moving in the opposite direction of the gradient. This step can be mathematically expressed as:</p>\n\n<p>$$\\theta= \\theta-\\alpha\\cdot\\nabla J(\\theta)$$</p>\n\n<p>where:</p>\n\n<ul>\n<li>$\\theta$: The parameters.</li>\n<li>$\\alpha$: The learning rate, which controls the step size.</li>\n<li>$\\nabla J(\\theta)$: The gradient of the loss function with respect to the parameters.</li>\n</ul></li>\n</ul>\n\n<h3>Types of Gradient Descent:</h3>\n\n<ul>\n<li><strong>Batch Gradient Descent</strong>: Uses the entire dataset to compute the gradient at each step. It's computationally expensive for large datasets but provides a stable convergence.</li>\n<li><strong>Stochastic Gradient Descent (SGD)</strong>: Uses a single data point to compute the gradient, leading to faster updates but more noise in convergence.</li>\n<li><strong>Mini-Batch Gradient Descent</strong>: A middle ground where a small subset of data (mini-batch) is used to compute the gradient. It balances efficiency and stability.</li>\n</ul>\n\n<h3>Challenges:</h3>\n\n<ul>\n<li>Choosing the Learning Rate: If it's too large, you may overshoot the minimum; if it's too small, convergence may be slow.</li>\n<li>Local Minima and Saddle Points: The algorithm might get stuck in a local minimum or a saddle point instead of finding the global minimum.</li>\n<li>Vanishing/Exploding Gradients: In deep networks, gradients can become too small or too large, hindering learning.</li>\n</ul>\n\n<h3>Extensions and Variants:</h3>\n\n<p>To address some challenges, variants like Momentum, RMSProp, and Adam add mechanisms to adapt the learning rate or smooth updates.</p>\n",
    "prev": "fill_missing",
    "next": "categorical_features"
}