{
    "question": "<p>Data Standardization</p>\n",
    "answer": "<p>Standardization and data scaling are essential preprocessing steps in many machine learning and statistical modeling tasks. These strategies ensure that features have comparable scales, which can improve the performance of many algorithms. Features with large ranges might overshadow others with smaller ranges. Standardization ensures that each feature contributes equally. Here are common strategies:</p>\n\n<h3>1. Standardization (Z-score Normalization)</h3>\n\n<ul>\n<li><strong>Definition</strong>: Transforms data to have a mean of zero and a standard deviation of one. $z = \\frac{x - \\mu}{\\sigma}$</li>\n<li><strong>Use When</strong>: The data follows a Gaussian (normal) distribution, and the goal is to ensure features are centered around zero with unit variance.</li>\n<li><strong>Applications</strong>: Linear regression, logistic regression, SVM, k-means clustering.</li>\n</ul>\n\n<h3>2. Min-Max Scaling (Normalization)</h3>\n\n<ul>\n<li><strong>Definition</strong>: Scales data to a fixed range, usually [0, 1] or [-1, 1]. $x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$</li>\n<li><strong>Use When</strong>: You want to scale features to a specific range.</li>\n<li><strong>Applications</strong>: Neural networks, algorithms requiring bounded input values.</li>\n</ul>\n\n<h3>3. Robust Scaling</h3>\n\n<ul>\n<li><strong>Definition</strong>: Scales data based on percentiles, making it robust to outliers. $x' = \\frac{x - \\text{median}(x)}{\\text{IQR}}$\n<strong>Use When</strong>: The data contains significant outliers, and you want a method that reduces the influence of outliers.</li>\n<li><strong>Applications</strong>: Any model sensitive to the scale of the data but robust to outliers.</li>\n</ul>\n\n<h3>4. MaxAbs Scaling</h3>\n\n<ul>\n<li><strong>Definition</strong>: Scales data to the range [-1, 1] by dividing each value by the maximum absolute value of the feature. $x' = \\frac{x}{\\max(|x|)}$</li>\n<li><strong>Use When</strong>: The data contains features with varying magnitudes, and you want to preserve zero entries.</li>\n<li><strong>Applications</strong>: Sparse data representations, such as TF-IDF matrices in text analysis.</li>\n</ul>\n\n<h3>5. Log Transformation</h3>\n\n<ul>\n<li><strong>Definition</strong>: Applies a logarithm to each data point to reduce skewness. $x' = \\log(x + 1)$ (adding 1 to avoid log(0) issues).</li>\n<li><strong>Use When</strong>: The data is highly skewed with long tails.</li>\n<li><strong>Applications</strong>: Financial data, biological data.</li>\n</ul>\n\n<h3>6. Box-Cox Transformation</h3>\n\n<ul>\n<li><strong>Definition</strong>: Transforms data to approximate normality by applying a power transformation. </li>\n</ul>\n\n<p>$$x' = \\begin{cases}\n\\frac{x^\\lambda - 1}{\\lambda} &amp; \\text{if } \\lambda \\neq 0 \\\n\\log(x) &amp; \\text{if } \\lambda = 0\n\\end{cases}$$</p>\n\n<ul>\n<li><strong>Use When</strong>: The data is not normally distributed, and normality is desired for modeling.</li>\n<li><strong>Applications</strong>: Regression analysis, ANOVA.</li>\n</ul>\n\n<h3>6. Yeo-Johnson Transformation</h3>\n\n<ul>\n<li><strong>Definition</strong>: The Yeo-Johnson transformation applies a piecewise-defined function to each data point $x$ in the dataset, with a parameter $\\lambda$ \nthat controls the nature of the transformation:</li>\n</ul>\n\n<p>$$\ny = \n\\begin{cases} \n\\frac{(x + 1)^\\lambda - 1}{\\lambda}, &amp; \\text{if } \\lambda \\neq 0, x \\geq 0 \\<br />\n\\ln(x + 1), &amp; \\text{if } \\lambda = 0, x \\geq 0 \\<br />\n\\frac{-(|x| + 1)^{2 - \\lambda} - 1}{2 - \\lambda}, &amp; \\text{if } \\lambda \\neq 2, x &lt; 0 \\<br />\n-\\ln(|x| + 1), &amp; \\text{if } \\lambda = 2, x &lt; 0<br />\n\\end{cases}\n$$</p>\n\n<ul>\n<li><strong>Use When</strong>: To make data more symmetric and closer to a normal distribution.\nTo stabilize variance when the data is heteroscedastic (i.e., the variance changes across the range of data).\nTo improve the performance of machine learning algorithms that assume normality or require scaled features. It can handle both positive and negative values, unlike the Box-Cox transformation.</li>\n</ul>\n",
    "prev": "curse_of_dimensionality",
    "next": "fill_missing"
}