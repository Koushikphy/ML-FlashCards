{
    "question": "<p>Bagging and Boosting</p>\n",
    "answer": "<p>Both bagging and boosting are ensemble methods in machine learning that combine multiple models to improve overall performance. However, they work in fundamentally different ways.</p>\n\n<h3>Bagging (Bootstrap Aggregating)</h3>\n\n<ol>\n<li><p><strong>How it Works</strong>:  </p>\n\n<ul>\n<li><strong>Data Resampling</strong>: Multiple subsets of the dataset are created by sampling with replacement (bootstrap sampling).  </li>\n<li><strong>Model Training</strong>: A separate model (usually of the same type, like decision trees) is trained on each subset.  </li>\n<li><strong>Aggregation</strong>: The final prediction is made by combining the outputs of all models, typically through averaging (for regression) or majority voting (for classification).</li>\n</ul></li>\n<li><p><strong>Key Features</strong>:  </p>\n\n<ul>\n<li>Models are trained <strong>independently</strong>.  </li>\n<li>Reduces <strong>variance</strong> by averaging predictions.  </li>\n<li>Best suited for high-variance, low-bias models (e.g., decision trees).</li>\n</ul></li>\n<li><p><strong>Example Algorithm</strong>:<br />\nRandom Forest is a popular bagging-based algorithm where multiple decision trees are trained on bootstrapped datasets, and each tree considers a random subset of features for splits.</p></li>\n</ol>\n\n<hr />\n\n<h3>Boosting</h3>\n\n<ol>\n<li><p><strong>How it Works</strong>:  </p>\n\n<ul>\n<li><strong>Sequential Learning</strong>: Models are trained one after another, and each new model corrects the errors of its predecessor.  </li>\n<li><strong>Weighted Updates</strong>: More emphasis is placed on the samples that were misclassified or poorly predicted by earlier models.  </li>\n<li><strong>Final Prediction</strong>: A weighted combination of all models' outputs is used to make the final prediction.</li>\n</ul></li>\n<li><p><strong>Key Features</strong>:  </p>\n\n<ul>\n<li>Models are trained <strong>sequentially</strong>.  </li>\n<li>Reduces both <strong>bias</strong> and <strong>variance</strong>.  </li>\n<li>Typically uses weak learners (e.g., shallow decision trees) and builds a strong learner by combining them.</li>\n</ul></li>\n<li><p><strong>Example Algorithms</strong>:  </p>\n\n<ul>\n<li><strong>AdaBoost (Adaptive Boosting)</strong>: Assigns higher weights to misclassified samples, forcing subsequent models to focus on these harder cases.  </li>\n<li><strong>Gradient Boosting</strong>: Optimizes the model by minimizing a loss function using gradient descent. Popular implementations include XGBoost, LightGBM, and CatBoost.</li>\n</ul></li>\n</ol>\n\n<table>\n<thead>\n<tr>\n  <th><strong>Aspect</strong></th>\n  <th><strong>Bagging</strong></th>\n  <th><strong>Boosting</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td><strong>Model Training</strong></td>\n  <td>Independent</td>\n  <td>Sequential</td>\n</tr>\n<tr>\n  <td><strong>Goal</strong></td>\n  <td>Reduce variance</td>\n  <td>Reduce bias and variance</td>\n</tr>\n<tr>\n  <td><strong>Focus</strong></td>\n  <td>Equal focus on all samples</td>\n  <td>Focus on difficult samples</td>\n</tr>\n<tr>\n  <td><strong>Combination</strong></td>\n  <td>Averaging or voting</td>\n  <td>Weighted sum</td>\n</tr>\n<tr>\n  <td><strong>Complexity</strong></td>\n  <td>Lower computational cost</td>\n  <td>Higher computational cost</td>\n</tr>\n<tr>\n  <td><strong>Overfitting</strong></td>\n  <td>Less prone to overfitting</td>\n  <td>May overfit if not regularized</td>\n</tr>\n<tr>\n  <td><strong>When to use</strong></td>\n  <td>Model has high variance</td>\n  <td>Model has high bias</td>\n</tr>\n</tbody>\n</table>\n",
    "prev": "handling_imbalance",
    "next": "decision_trees"
}