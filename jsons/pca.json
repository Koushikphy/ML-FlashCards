{
    "question": "<p>Principal Component Analysis (PCA)</p>\n",
    "answer": "<p>Principal Component Analysis (PCA) is a dimensionality reduction technique often used in data analysis and machine learning to reduce the complexity of data while retaining most of its variability. Here's how I would explain it during an interview:</p>\n\n<h3>What is PCA?</h3>\n\n<p>PCA is a statistical technique used to simplify high-dimensional datasets by transforming them into a smaller set of uncorrelated variables called principal components. These components capture the maximum variance in the data.</p>\n\n<h3>Why use PCA?</h3>\n\n<ul>\n<li><strong>Dimensionality Reduction:</strong> Simplifies datasets with many features, making them easier to analyze and visualize.</li>\n<li><strong>Feature Extraction:</strong> Removes redundant information by combining correlated features into a single component.</li>\n<li><strong>Noise Reduction:</strong> Helps eliminate less informative components that may be noise.</li>\n<li><strong>Improved Model Performance:</strong> Can reduce overfitting in machine learning models.</li>\n</ul>\n\n<h3>How does PCA work?</h3>\n\n<ol>\n<li><p><strong>Standardize the Data</strong><br />\nSince PCA is affected by scale, we standardize the features to have a mean of 0 and variance of 1.</p></li>\n<li><p><strong>Compute the Covariance Matrix</strong><br />\nA covariance matrix is created to understand the relationships between features.</p></li>\n<li><p><strong>Calculate Eigenvalues and Eigenvectors</strong>  </p>\n\n<ul>\n<li>Eigenvalues measure the amount of variance captured by each principal component.  </li>\n<li>Eigenvectors define the directions of the components.</li>\n</ul></li>\n<li><p><strong>Select Principal Components</strong><br />\nRank eigenvalues in descending order and select the top $k$ components that explain the majority of the variance.</p></li>\n<li><p><strong>Transform the Data</strong><br />\nProject the original data onto the new $k$-dimensional space using the selected principal components.</p></li>\n</ol>\n\n<h3>Example Scenario</h3>\n\n<p>Imagine a dataset with 100 features. Many features might be correlated and redundant. PCA can reduce these 100 features to, say, 10 principal components, while retaining most of the information. This can make data visualization (e.g., scatter plots) feasible in 2D or 3D.</p>\n\n<h3>Key Points to Remember</h3>\n\n<ul>\n<li>PCA is unsupervised: It does not consider the target variable.</li>\n<li>It assumes linear relationships and maximizes variance.</li>\n<li>The number of components chosen is often based on cumulative variance (e.g., keeping components that explain 95% of the variance).</li>\n</ul>\n\n<h3>When Not to Use PCA</h3>\n\n<ul>\n<li>When interpretability of features is critical, as PCA creates new, less interpretable features.</li>\n<li>If the dataset is non-linear and PCA fails to capture complex relationships. Kernel PCA or t-SNE might be better alternatives.</li>\n</ul>\n\n<p>By emphasizing its purpose, process, and limitations, this explanation provides both technical depth and clarity suitable for an interview.</p>\n",
    "prev": "fill_missing",
    "next": "gradient_descent"
}