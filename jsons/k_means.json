{
    "question": "<p>K-Means Clustering</p>\n",
    "answer": "<p>K-Means is an unsupervised machine learning algorithm used for partitioning data into <strong>K clusters</strong>, where each cluster is represented by its centroid (mean point).</p>\n\n<h3>How it works</h3>\n\n<ol>\n<li><p><strong>Initialize Centroids</strong>: Select $K$ random points from the data as initial centroids.</p></li>\n<li><p><strong>Assign Points to Clusters</strong>:For each data point, calculate its distance to all centroids (commonly using Euclidean distance).Assign the point to the cluster of the closest centroid.</p></li>\n<li><p><strong>Update Centroids</strong>: Calculate the mean of all points in each cluster, and update the centroid to this mean value.</p></li>\n<li><p><strong>Repeat</strong>: Repeat the assignment and update steps until convergence i.e., when centroids no longer change significantly or a specified number of iterations is reached.</p></li>\n</ol>\n\n<h3>Key points:</h3>\n\n<ol>\n<li><p><strong>Distance Metric</strong>: Euclidean distance is commonly used to measure the distance between data points and centroids, but other metrics can be used depending on the application.</p></li>\n<li><p><strong>Applications</strong>: K-means clustering is widely used in various fields, including customer segmentation, image segmentation, document clustering, and anomaly detection.</p></li>\n<li><p><strong>Evalutation metric</strong> The Silhouette Coefficient is calculated using the mean intra-cluster distance (<code>a</code>) and the mean nearest-cluster distance (<code>b</code>) for each sample. The Silhouette Coefficient for a sample is <code>(b - a) / max(a, b)</code>.</p></li>\n<li><p><strong>Choosing $K$: The Elbow Method</strong>: Plot the total within-cluster sum of squares (inertia) against different values of $K$. Look for the \"elbow\" point where the rate of decrease slows down. This is often the optimal number of clusters.</p></li>\n</ol>\n\n<h3>Advantages</h3>\n\n<ul>\n<li>Simple and easy to implement.</li>\n<li>Scalable to large datasets.</li>\n<li>Works well when clusters are well-separated.</li>\n</ul>\n\n<h3>Disadvantages</h3>\n\n<ul>\n<li>Requires specifying $ K $ beforehand.</li>\n<li>Sensitive to:\n<ul>\n<li>Initial centroid placement (can lead to different results).</li>\n<li>Outliers, which can skew clusters. Techniques like K-means++ are often used to improve initialization.</li>\n</ul></li>\n<li>Assumes clusters are spherical and evenly sized, which may not hold in real-world data.</li>\n</ul>\n",
    "prev": "k_nearest_neighbour",
    "next": "confusion_matrix"
}