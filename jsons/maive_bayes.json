{
    "question": "<p>Naive Bayes Classifier</p>\n",
    "answer": "<p>The <strong>Naive Bayes classifier</strong> is a machine learning algorithm based on Bayes' Theorem, particularly suitable for classification tasks. It assumes that the features (input variables) are <strong>conditionally independent</strong> given the class label. This \"naive\" assumption makes the computation much simpler and more efficient, even though it may not hold perfectly in practice.</p>\n\n<h4><strong>Steps in Naive Bayes Classification</strong></h4>\n\n<ol>\n<li><p><strong>Compute Priors</strong>:\nCalculate the prior probability for each class $P(C)$, where $C$ is the class label.</p></li>\n<li><p><strong>Compute Likelihood</strong>:\nCalculate the likelihood $P(X_i|C)$ for each feature $X_i$ in the dataset, given the class $C$.</p></li>\n<li><p><strong>Apply Bayes' Theorem</strong>:\nUse Bayes' Theorem to compute the posterior probability for each class:\n$$   P(C|X) \\propto P(C) \\prod_{i} P(X_i|C)$$</p>\n\n<p>Here, $P(C|X)$ is the posterior probability of class $C$ given the feature vector $X = (X_1, X_2, \\dots, X_n)$.</p></li>\n<li><p><strong>Predict Class</strong>:\nChoose the class with the highest posterior probability:\n$$   \\text{Predicted Class} = \\arg\\max_C P(C|X)$$</p></li>\n</ol>\n\n<h4><strong>Advantages</strong></h4>\n\n<ul>\n<li>Simple and computationally efficient.</li>\n<li>Works well with high-dimensional data.</li>\n<li>Performs well with categorical data and text classification (e.g., spam filtering).</li>\n</ul>\n\n<h4><strong>Disadvantages</strong></h4>\n\n<ul>\n<li>The assumption of conditional independence is often unrealistic.</li>\n<li>Performs poorly when features are highly correlated or when data is insufficient.</li>\n</ul>\n\n<h4>Common Use Cases</h4>\n\n<ul>\n<li><strong>Text Classification</strong>: Spam detection, sentiment analysis.</li>\n<li><strong>Medical Diagnosis</strong>: Predicting diseases based on symptoms.</li>\n<li><strong>Recommender Systems</strong>: Suggesting products based on user behavior.</li>\n</ul>\n\n<h4>Example</h4>\n\n<p>Suppose we want to classify an email as \"Spam\" or \"Not Spam\" based on the occurrence of certain words. Using Naive Bayes:  </p>\n\n<ol>\n<li>Compute prior probabilities ($P(\\text{Spam})$, $P(\\text{Not Spam})$).  </li>\n<li>Compute likelihoods ($P(\\text{Word}| \\text{Spam})$, $P(\\text{Word}| \\text{Not Spam})$).  </li>\n<li>Calculate posterior probabilities for each class given the words in the email.  </li>\n<li>Predict the class with the highest posterior probability.  </li>\n</ol>\n",
    "prev": "bayes",
    "next": "cross_validation"
}