{
    "question": "<p>Bias-Variance tradeoff</p>",
    "answer": "<ol>\n<li><strong>Bias</strong>: Bias refers to the error introduced by approximating a real-world problem with a simplified model. A high bias model is overly simplistic and tends to underfit the data, failing to capture important patterns and trends.</li>\n<li><strong>Variance</strong>: Variance measures the model's sensitivity to small fluctuations in the training data. A high variance model is complex and flexible, fitting the training data very closely but potentially overfitting noise and outliers.</li>\n<li><strong>Tradeoff</strong>: The bias-variance tradeoff implies that decreasing bias typically increases variance, and vice versa. The goal is to find a model that strikes a balance between bias and variance to achieve optimal predictive performance on new, unseen data.</li>\n<li><strong>Implications</strong>:<ul>\n<li><strong>Underfitting</strong>: Models with high bias and low variance tend to underfit the training data, resulting in poor performance on both training and test datasets.</li>\n<li><strong>Overfitting</strong>: Models with low bias and high variance may overfit the training data, performing well on training data but poorly on test data due to capturing noise.</li>\n</ul>\n</li>\n<li><strong>Model Selection</strong>: To find the optimal balance, techniques such as cross-validation, regularization, and ensemble methods (like bagging and boosting) are used:<ul>\n<li><strong>Regularization</strong>: Introduces a penalty to the model complexity to reduce variance.</li>\n<li><strong>Ensemble Methods</strong>: Combine multiple models to reduce variance while maintaining low bias</li>\n</ul>\n</li>\n</ol>\n<p>The relationship between the bias of an estimator and its variance. Total prediction error = Bias$^2$+Variance+Iruducible error.</p>",
    "prev": "logistic_regression",
    "next": "confusion_matrix"
}