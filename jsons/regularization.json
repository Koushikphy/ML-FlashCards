{
    "question": "<p>Regularization</p>\n",
    "answer": "<p>Regularization techniques in regression analysis are methods used to prevent overfitting and improve the generalization ability of models by adding a penalty to the loss function. The benefits of regularization techniques include improved model interpretability, reduced variance, and enhanced predictive performance on unseen data. </p>\n\n<p>The two main types of regularization techniques are:</p>\n\n<ol>\n<li><strong>Ridge Regression (L2 Regularization)</strong>:\n<ul>\n<li>Ridge regression adds a penalty term proportional to the square of the coefficients (L2 norm) to the ordinary least squares (OLS) objective function ⇒ $\\text{Loss} + \\lambda \\sum_{j=1}^{p} \\beta_j^2$</li>\n<li>Ridge regression is widely used in situations where multicollinearity is present or when the number of predictors (features) is large. It is a fundamental tool in regression analysis and machine learning for improving model robustness and interpretability.</li>\n</ul></li>\n<li><strong>Lasso Regression (L1 Regularization)</strong>:\n<ul>\n<li>Lasso regression adds a penalty term proportional to the absolute value of the coefficients (L1 norm) to the objective function ⇒ $\\text{Loss} + \\lambda \\sum_{j=1}^{p} |\\beta_j|$</li>\n<li>It is particularly useful when dealing with high-dimensional datasets where many predictors may not contribute significantly to the model.</li>\n<li>Unlike ridge regression, lasso can shrink coefficients all the way to zero, effectively performing feature selection by eliminating less important predictors.</li>\n</ul></li>\n</ol>\n",
    "prev": "bias_variance_tradeoff",
    "next": "gradient_descent"
}