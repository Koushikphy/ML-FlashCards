## üß† **Section 1: NLP & GenAI Fundamentals**

### **1. What is the difference between NLP and Generative AI (GenAI)?**

**NLP (Natural Language Processing)** is a **broad field** within AI that focuses on enabling machines to understand, interpret, and respond to human language. It includes tasks such as:

* **Text classification** (e.g., spam detection)
* **Named Entity Recognition** (e.g., detecting "New York" as a location)
* **Sentiment analysis** (e.g., identifying opinions or emotions)
* **Machine translation** (e.g., English to French)

**Generative AI**, on the other hand, refers specifically to AI models that are capable of **producing new content** ‚Äî such as text, images, audio, or code. It often **uses NLP as a component** when working with language.

Think of it like this:

| Feature           | NLP                                             | GenAI                                                    |
| ----------------- | ----------------------------------------------- | -------------------------------------------------------- |
| **Purpose**       | Understand and process language                 | Create new content                                       |
| **Includes**      | Information extraction, syntactic analysis      | Text generation, image synthesis                         |
| **Example Tools** | spaCy, NLTK, CoreNLP                            | ChatGPT, DALL¬∑E, Gemini                                  |
| **Relationship**  | NLP is a subset of the techniques used in GenAI | GenAI is broader and includes NLP, computer vision, etc. |

> For instance, ChatGPT uses NLP tasks like parsing and entity recognition internally, but it‚Äôs ultimately a **generative model**.

---

### **2. What is tokenization? How is it different from lemmatization?**

**Tokenization** is the first step in most NLP pipelines. It breaks down raw text into **individual units called tokens**. These could be:

* **Words** (e.g., ‚Äúchat‚Äù, ‚ÄúGPT‚Äù)
* **Subwords** (e.g., ‚Äúun-‚Äù, ‚Äúbreak‚Äù, ‚Äúable‚Äù)
* **Characters**

Example:

```text
"I love natural language processing."
```

Tokenized ‚Üí `["I", "love", "natural", "language", "processing", "."]`

---

**Lemmatization**, on the other hand, is about **reducing words to their root or base form**, called a **lemma**. Unlike stemming (which just chops suffixes), lemmatization uses linguistic knowledge such as parts of speech and dictionary lookups.

Example:

* `"running"` ‚Üí `"run"`
* `"studies"` ‚Üí `"study"`
* `"better"` ‚Üí `"good"` *(semantic normalization)*

| Aspect                   | Tokenization            | Lemmatization                      |
| ------------------------ | ----------------------- | ---------------------------------- |
| Output                   | Tokens (words/subwords) | Root/dictionary forms              |
| Uses grammar rules?      | No                      | Yes                                |
| Position in NLP pipeline | First step              | After tokenization and POS tagging |

So:
üëâ Tokenization *splits* text,
üëâ Lemmatization *normalizes* words.

---

### **3. Which method is used to remove stopwords?**

**Stopwords** are common words in a language that **carry little meaning** on their own ‚Äî like ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúan‚Äù, ‚Äúin‚Äù. Removing them reduces noise and improves efficiency.

**How to remove stopwords**:

1. **Tokenize the text**
2. **Use a predefined list of stopwords**
3. **Filter them out**

**Example using Python:**

```python
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

text = "This is a simple example to remove stopwords."
tokens = word_tokenize(text.lower())
filtered = [word for word in tokens if word not in stopwords.words('english')]
```

Stopword lists can be customized based on the application. For example, in sentiment analysis, words like ‚Äúnot‚Äù may be **important** and should not be removed.

---

### **4. What is Named Entity Recognition (NER)?**

**Named Entity Recognition (NER)** is a technique in NLP that **automatically detects and classifies entities** in text into predefined categories such as:

* **Person** (e.g., "Elon Musk")
* **Organization** (e.g., "Google")
* **Location** (e.g., "India")
* **Date/Time**, **Money**, etc.

It is used in:

* **Information extraction**
* **Question answering**
* **Search engines**
* **Document classification**

**Example:**

```text
"Barack Obama was born in Hawaii in 1961."
```

NER Output:

* `"Barack Obama"` ‚Üí PERSON
* `"Hawaii"` ‚Üí LOCATION
* `"1961"` ‚Üí DATE

NER uses machine learning (e.g., CRFs, LSTMs, BERT) to **learn patterns from annotated corpora**.

---

### **5. What is semantic search?**

**Semantic search** improves traditional keyword-based search by **understanding the intent and contextual meaning** of queries and documents.

Instead of matching exact words, it matches **meanings** using:

* **Embeddings** (vector representations)
* **Transformer models** (e.g., BERT, SBERT)

**Why it's powerful:**

* Handles **synonyms**, **rephrased queries**
* Understands **user intent**
* Useful in **Q\&A systems**, **document search**, **chatbots**

**Example:**

Query:

> "How can I increase muscle strength?"

Traditional search may look for ‚Äúincrease‚Äù + ‚Äúmuscle‚Äù + ‚Äústrength‚Äù.

Semantic search could find:

> ‚ÄúBest exercises to build stronger muscles‚Äù ‚Äî even if no keywords exactly match.

---

### **6. What are embeddings?**

**Embeddings** convert textual information (words, sentences, or documents) into **dense numerical vectors** that represent **semantic meaning**.

Why are they important?

* Machines can‚Äôt ‚Äúunderstand‚Äù text ‚Äî they need numbers.
* Embeddings capture **context, similarity, and structure** of language.

There are several types:

| Type                      | Example         | Description                       |
| ------------------------- | --------------- | --------------------------------- |
| **Word embeddings**       | Word2Vec, GloVe | Fixed-length vectors for words    |
| **Contextual embeddings** | BERT, GPT       | Word meaning changes with context |
| **Sentence embeddings**   | SBERT           | Represent full sentences          |

**Applications**:

* Semantic search
* Text classification
* Clustering
* Recommendation systems

> Similar meanings ‚Üí similar vectors
> `"Paris"` and `"France's capital"` might be close in vector space.


Embeddings convert text‚Äîtypically words, subwords, or tokens‚Äîinto dense numerical vectors that can be processed by machine learning models. These vectors capture semantic information, allowing similar words to have similar vector representations.

Basic embeddings, such as Word2Vec, GloVe, or FastText, use a fixed lookup table. Each word is assigned a unique vector based on patterns learned during training, typically by analyzing word co-occurrence in large text corpora. Once trained, the vector for a word is always the same‚Äîregardless of context. For example, the word "bank" will have the same vector whether it's used in the sense of "river bank" or "financial bank." These are called static embeddings.

In contrast, modern transformer-based models like BERT, GPT, and similar architectures start with an embedding layer too‚Äîinitially mapping each token to a basic vector. However, these vectors are then passed through multiple transformer layers. These layers use mechanisms like self-attention to capture relationships between tokens in a sentence, allowing the model to understand context. As a result, the embedding for a word like "bank" can change depending on whether it's used in a financial or geographical context.

This process produces contextualized embeddings, which are more powerful because they reflect not just the identity of a word, but also its role and meaning within a specific sentence.


---

### **7. How does a tokenizer handle out-of-vocabulary (OOV) or unknown words?**

Different types of tokenizers handle **unknown words** differently:

#### 1. **Word-level tokenizers**:

* Maintain a dictionary of known words.
* Unknown words become `[UNK]` (unknown token).
* Problem: Many OOV words ‚Üí model failure.

#### 2. **Subword-level tokenizers**:

* **Byte-Pair Encoding (BPE)**, **WordPiece**, or **SentencePiece**
* Break unknown words into **smaller known units**.
* Reduces OOV drastically.

**Example:**
Word: `"unhappiness"`

Using WordPiece:
‚Üí `["un", "##happi", "##ness"]`
Using BPE:
‚Üí `["un", "hap", "pi", "ness"]`

Even if `"unhappiness"` isn't in the vocabulary, we can build it from known parts.

> This is why models like **BERT and GPT** rarely encounter OOV tokens.

---
## ‚öôÔ∏è **Section 2: Transformer and LLM Architectures**

### **8. Explain the Transformer architecture.**

The **Transformer** is a deep learning model architecture introduced by Vaswani et al. in 2017 in the paper *"Attention is All You Need"*. It's specifically designed for handling sequential data like language but does so **without using recurrence (RNNs or LSTMs)**. This allows it to **process sequences in parallel**, making training much faster.

**Key Components:**

* **Input Embeddings**: Converts input tokens (words, subwords) into dense vectors.
* **Positional Encoding**: Since Transformers don‚Äôt process tokens in order, positional encodings are added to embeddings to retain word order.
* **Self-Attention Mechanism**: Allows each token to weigh other tokens in the sequence based on relevance.
* **Multi-Head Attention**: The self-attention mechanism is applied multiple times in parallel to allow the model to learn from different perspectives.
* **Feed-Forward Network (FFN)**: Applies the same neural network to each position independently after attention.
* **Residual Connections & Layer Normalization**: Stabilize and speed up training.

**Architecture Overview:**

* **Encoder Stack** (used in BERT):

  * Each layer = Multi-head self-attention + FFN
  * Input processed all at once
* **Decoder Stack** (used in GPT):

  * Each layer = Causal self-attention + FFN
  * Generates tokens one by one
* **Encoder-Decoder Stack** (used in T5, BART):

  * Encoder processes input; decoder generates output while attending to encoder output.

---

### **9. Explain attention in Transformers.**

**Attention** is a mechanism that allows the model to decide which parts of the input are important for generating a particular part of the output. It replaces recurrence and helps the model handle long-term dependencies.

#### **Self-Attention Explained:**

Each token gets converted into 3 vectors:

* **Query (Q)**
* **Key (K)**
* **Value (V)**

The **attention score** is calculated by:

```
Attention(Q, K, V) = softmax(QK·µÄ / ‚àöd_k) * V
```

* Dot product `QK·µÄ`: Measures similarity between tokens.
* `softmax`: Converts scores to probabilities.
* Result: A weighted sum of value vectors (V), where weights come from attention scores.

**Multi-head attention** = Runs this attention process in parallel with different projections to learn different relationships.

---

### **10. Why are Transformers better than LSTMs?**

Transformers outperform LSTMs for several reasons:

| Feature                   | LSTM                          | Transformer                        |
| ------------------------- | ----------------------------- | ---------------------------------- |
| **Parallelization**       | Sequential, slower training   | Parallelizable across sequences    |
| **Memory of Past Tokens** | Limited (vanishing gradients) | Global attention ‚Äî sees all tokens |
| **Long-Range Dependency** | Poor handling                 | Handles easily via attention       |
| **Scalability**           | Hard to scale                 | Scales well with GPUs and data     |
| **Statefulness**          | Maintains hidden state        | Stateless, easier to manage        |

Also, transformers form the basis of models like BERT, GPT, T5 ‚Äî all of which dominate modern NLP benchmarks.

---

### **11. Difference between encoder-only, decoder-only, and encoder-decoder LLMs?**

#### **Encoder-only** (e.g., **BERT**):

* Used for understanding tasks like classification, NER.
* Looks at the full input at once (bidirectional attention).
* Not suitable for text generation.

#### **Decoder-only** (e.g., **GPT**):

* Used for generation tasks like chat, text continuation.
* Uses causal attention (only looks at previous tokens).
* Predicts one token at a time, auto-regressively.

#### **Encoder-Decoder** (e.g., **T5**, **BART**):

* Encoder processes input and creates context.
* Decoder uses context to generate output.
* Suitable for tasks like translation, summarization.

---

### **12. Causal LLM, Autoregressive, Autoencoding, and Others**

| Type                           | Description                                                 | Example  |
| ------------------------------ | ----------------------------------------------------------- | -------- |
| **Causal LLM**                 | Only attends to past tokens; avoids looking into the future | GPT      |
| **Autoregressive LLM**         | Predicts next token based on prior ones                     | GPT      |
| **Autoencoding LLM**           | Learns to reconstruct full input or masked parts            | BERT     |
| **Encoder-Decoder (Seq2Seq)**  | Encodes input, decodes output                               | T5, BART |
| **PrefixLM**                   | Attends to a known prefix, then generates                   | UL2      |
| **Retrieval-Augmented Models** | Use external knowledge/document store                       | RAG      |

---

### **13. Base vs. Instruct Variants of LLMs**

* **Base LLMs**:

  * Trained on large corpora to predict next tokens.
  * No specific task alignment (general purpose).
  * E.g., GPT-3, LLaMA base.

* **Instruct LLMs**:

  * Fine-tuned using datasets with human instructions.
  * Trained to follow commands like ‚ÄúSummarize this‚Ä¶‚Äù
  * Often includes RLHF (Reinforcement Learning from Human Feedback).
  * E.g., ChatGPT, LLaMA-2-chat, Mistral-Instruct.

---

### **14. Difference Between Training and Inference**

| Phase     | Training                              | Inference                            |
| --------- | ------------------------------------- | ------------------------------------ |
| Purpose   | Learn patterns                        | Make predictions                     |
| Gradients | Calculated and used to update weights | Not used                             |
| Data Flow | Forward + Backward pass               | Only forward pass                    |
| Speed     | Slower, more compute intensive        | Faster                               |
| Example   | Teaching a model English              | Using the model to answer a question |

---

### **15. What is Gradient Accumulation?**

When GPU memory is limited and you can't use large batch sizes, **gradient accumulation** helps.

How it works:

1. Run multiple small mini-batches.
2. Accumulate gradients across batches.
3. Update model weights **after N mini-batches**, as if you trained with one large batch.

This allows:

* Better performance
* Use of limited hardware
* More stable training

---

### **16. Explain: forward pass, cross-entropy loss, and safetensor**

* **Forward Pass**: Data is passed through the model layer by layer to get predictions.
* **Cross-Entropy Loss**: Measures difference between predicted probabilities and true labels. High when predictions are wrong, low when correct. Common in classification.

  ```math
  Loss = -Œ£ (true label * log(predicted prob))
  ```
* **Safetensor**: A secure and fast format for saving model weights.

  * Replaces `.pt` or `.bin`
  * Prevents execution of arbitrary code on load
  * Often used in HuggingFace and secure deployments

---

### **17. What are Pad Tokens and Why Are They Needed?**

* In NLP, input sequences have different lengths (e.g., ‚ÄúHi‚Äù vs. ‚ÄúHow are you?‚Äù).
* **Pad tokens** (like `[PAD]`) are used to make all sequences in a batch the same length.
* Needed for batching and matrix multiplication efficiency.
* **Attention masks** are used to ignore pad tokens during model training.

Example:

```
Input 1: [CLS] Hello world [SEP] [PAD]
Input 2: [CLS] Hi [SEP] [PAD] [PAD]
```

---

### **18. What is JSONL and How Is It Used in NLP?**

**JSONL (JSON Lines)** is a file format where each line is a JSON object. It‚Äôs commonly used for large datasets because it‚Äôs easy to read and stream line-by-line.

Example:

```json
{"prompt": "Translate to French: Hello", "completion": "Bonjour"}
{"prompt": "Summarize: AI is...", "completion": "AI is the study of..."}
```

**Used For:**

* Dataset storage (e.g., for LLM fine-tuning)
* Logging predictions
* Feeding data into training scripts (especially HuggingFace/Dataset library)

---

## üß© **Section 3: LLM Prompting & Usage**

### **19. What is a context window in LLMs?**

The **context window** (also called **context length** or **sequence length**) refers to the **maximum number of tokens** an LLM can read and attend to at once during inference or training.

* For example, GPT-3 has a 2,048-token context window, GPT-4-turbo goes up to 128,000.
* Tokens are not the same as words; for example, ‚Äúunbelievable‚Äù might be broken into `["un", "believable"]` (2 tokens), depending on the tokenizer.

**Why it matters:**

* The **context window** limits how much the model can "remember" during a session.
* If the input exceeds the window, the oldest tokens are dropped (or truncated).
* Larger context windows enable the model to:

  * Read long documents
  * Handle long conversations
  * Retain more context in code, legal documents, etc.

---

### **20. Why do prompts have different roles like system, user, and assistant?**

In **chat-based LLMs** (like ChatGPT), prompts are often divided into **roles** to simulate a structured conversation:

* **System role**: Sets behavior, tone, or personality of the assistant.

  * Example: "You are a helpful and concise tutor."
* **User role**: Represents human input.

  * Example: "What is the difference between AI and ML?"
* **Assistant role**: Represents the model‚Äôs response.

This **role-based structure** provides context and control:

* Enables **persona control** (e.g., formal vs. friendly tone)
* Improves **task alignment** (e.g., writing code vs. explaining a poem)
* Helps the model infer **turn-based structure**, which is essential for chat-style interaction

---

### **21. What are tools in LLM prompts?**

**Tools (aka functions, plugins, or APIs)** in LLMs allow the model to **call external systems or APIs** to retrieve or process information **beyond its training data**.

**Example Tools:**

* **Calculator**: For solving math problems
* **Search**: To look up real-time data (e.g., current weather)
* **Python**: Execute code
* **Web Browsing or Database Access**: Pull live information

**How it works:**

* Prompt: "What is the current price of Bitcoin?"
* LLM doesn't know real-time data, so it calls the `search` tool, retrieves the answer, and returns it as a reply.

**Benefit:** Extends the LLM‚Äôs capabilities beyond its static knowledge.

---

### **22. Why does Hugging Face AutoTokenizer return numbers instead of words or word pieces?**

Hugging Face‚Äôs `AutoTokenizer` converts text into **token IDs** (integers), because neural networks can only process numbers.

Here‚Äôs the pipeline:

```text
Input: "Hello world"
‚Üì
Tokenized: ['Hello', 'world']
‚Üì
IDs: [15496, 995]
```

**Why numbers?**

* Each token has a corresponding ID in the **vocabulary**.
* These IDs are used to **look up embedding vectors** during the forward pass.

To reverse the process:

```python
tokenizer.decode([15496, 995])  # "Hello world"
```

If you're seeing numbers, that's the model's internal representation of your text.

---

### **23. What are system prompts and guardrails in production LLMs?**

#### **System Prompts**:

These are special instructions given to LLMs at the **start of a session** to influence their behavior.

* Example: `"You are a safety-focused assistant. Do not give harmful advice."`
* Not visible to users.
* Helps define tone, goals, rules, safety instructions.

#### **Guardrails**:

Guardrails are **restrictions or filters** that limit what the model is allowed to say or do.

* They can be implemented via:

  * Prompt patterns
  * Content moderation APIs
  * Output filtering (e.g., toxicity checks)
  * Tool usage limits (e.g., disabling code execution in unsafe cases)

**Goal:** Make the LLM **safe**, **compliant**, and **aligned** for production use (e.g., healthcare, education, finance).

---

### **24. What are prompt engineering techniques (e.g., zero-shot, few-shot, chain-of-thought)?**

Prompt engineering is the practice of **designing inputs** to maximize model performance without fine-tuning.

#### **Zero-shot prompting**:

* No examples provided.
* Relies on model‚Äôs general capabilities.
* Example:
  *Prompt:* ‚ÄúTranslate this to French: I love you.‚Äù
  *Output:* ‚ÄúJe t‚Äôaime‚Äù

#### **Few-shot prompting**:

* Gives **a few examples** in the prompt.
* Helps guide the model on task formatting.
* Example:
  *Prompt:*
  `English: Hello ‚Üí French: Bonjour`
  `English: Goodbye ‚Üí French:`
  *Model outputs:* `Au revoir`

#### **Chain-of-thought (CoT)** prompting:

* Encourages the model to **reason step-by-step**.
* Example:
  *Prompt:*
  ‚ÄúIf John has 3 apples and buys 2 more, how many apples does he have? Let‚Äôs think step by step.‚Äù
  *Output:*
  ‚ÄúJohn starts with 3 apples. He buys 2 more. 3 + 2 = 5. So he has 5 apples.‚Äù

#### **Others:**

* **Instruction prompting**: ‚ÄúExplain like I‚Äôm 5.‚Äù
* **Role prompting**: ‚ÄúYou are an expert lawyer. Explain this case.‚Äù
* **Reflexion prompting**: Ask the model to critique or refine its own output.
* **Self-consistency**: Sample multiple answers and select the most consistent one.

---

## üîç **Section 4: LLM Evaluation and Selection**

### **25. What are the key factors to consider when choosing an LLM for a specific use case?**

Selecting the right LLM involves balancing **capability, cost, safety, and constraints**. Key factors include:

#### ‚úÖ **1. Task Type**

* **Text generation, classification, summarization, translation** ‚Üí Use encoder-decoder or decoder-only (T5, GPT).
* **Understanding/classification, NER, embeddings** ‚Üí Encoder-only (BERT).

#### ‚úÖ **2. Latency and Inference Cost**

* Large models (e.g., GPT-4) are more accurate but **slower and expensive**.
* Smaller models (e.g., Mistral, LLaMA-2 7B) are **faster and cheaper** but might lack depth.

#### ‚úÖ **3. Context Window Requirements**

* Long documents or conversations? Choose models with long context support (GPT-4 Turbo, Claude 3).

#### ‚úÖ **4. Multimodal Needs**

* Need image + text or video understanding? Go for multimodal LLMs (e.g., Gemini, GPT-4V, Claude 3 Opus).

#### ‚úÖ **5. Availability / Open vs. Closed**

* Open-source (LLaMA, Mistral, Falcon): more customizable
* Closed (GPT, Claude): better performance, but less control

#### ‚úÖ **6. Alignment & Safety**

* Regulated industries (finance, healthcare): use models with strong safety controls and guardrails.

#### ‚úÖ **7. Tool and Plugin Support**

* If tool calling is required (search, database access), choose models that support these features (e.g., OpenAI tools, LangChain-compatible).

---

### **26. Difference between GPT and BERT ‚Äî When to use each?**

| Feature          | **GPT (Generative Pre-trained Transformer)** | **BERT (Bidirectional Encoder Representations from Transformers)** |
| ---------------- | -------------------------------------------- | ------------------------------------------------------------------ |
| **Architecture** | Decoder-only                                 | Encoder-only                                                       |
| **Training**     | Predicts next token (causal)                 | Predicts masked tokens (masked language modeling)                  |
| **Attention**    | Unidirectional (left to right)               | Bidirectional                                                      |
| **Best for**     | Text generation, chatbots, coding            | Text understanding, classification, embeddings                     |
| **Examples**     | GPT-3, GPT-4, ChatGPT                        | BERT, RoBERTa, DistilBERT                                          |

‚úÖ **Use BERT** for:

* Sentiment analysis
* Named Entity Recognition
* Semantic similarity

‚úÖ **Use GPT** for:

* Story generation
* Summarization
* Answering questions, coding, creative tasks

---

### **27. Differences between Text-to-Text and Multimodal Generation**

| Type         | **Text-to-Text**           | **Multimodal**                                     |
| ------------ | -------------------------- | -------------------------------------------------- |
| **Input**    | Text                       | Text + image, audio, video, etc.                   |
| **Output**   | Text                       | Text, image, audio, etc.                           |
| **Examples** | Translation, summarization | Image captioning, image generation, VQA            |
| **Models**   | T5, GPT-3, BERT            | GPT-4V, Gemini, Flamingo, DALL¬∑E, Stable Diffusion |

**Multimodal models** are more complex and allow for tasks like:

* ‚ÄúDescribe this image‚Äù
* ‚ÄúGenerate an image from this prompt‚Äù
* ‚ÄúWhat is in this video?‚Äù

---

### **28. Are image generation models considered large language models (LLMs)?**

**No**, not exactly.

* Image generation models like **Stable Diffusion, DALL¬∑E, Midjourney** are **not LLMs**, because they don‚Äôt primarily operate on **language**.
* They are **generative models**, often using **Transformers or Diffusion architectures**, but trained on **image‚Äìtext pairs**.
* Some models **combine LLMs with image generation**, e.g.:

  * **DALL¬∑E**: uses GPT-like text encoder with a VAE/diffusion-based image generator.
  * **GPT-4V**: combines visual and language processing.

In summary:

* LLM = large model trained primarily on language
* Image gen = large model trained on images or image‚Äìtext, often **not classified as LLMs**

---

### **29. Transformers vs. Diffusers ‚Äì How do they compare and when are they used?**

| Aspect           | **Transformers**                                   | **Diffusers**                                    |
| ---------------- | -------------------------------------------------- | ------------------------------------------------ |
| **Used in**      | NLP, vision, audio                                 | Image generation, denoising                      |
| **How it works** | Attention mechanism models all-token relationships | Gradually denoises random noise to generate data |
| **Output Type**  | Sequences (tokens, words)                          | Continuous data (images, audio)                  |
| **Speed**        | Fast inference, slow training                      | Slower inference, high quality                   |
| **Examples**     | GPT, BERT, T5                                      | Stable Diffusion, Imagen, DALL¬∑E 2               |

**When to use:**

* Use **Transformers** for tasks involving **text** (translation, summarization).
* Use **Diffusers** for tasks involving **image/audio synthesis** (text-to-image, inpainting).

**Some newer architectures combine both**, e.g., **PixArt**, **Versatile Diffusion**.

---

### **30. Common Metrics to Evaluate LLM Output Quality**

#### **Automatic Metrics**:

| Metric                     | Purpose                                      | Used For            |
| -------------------------- | -------------------------------------------- | ------------------- |
| **BLEU**                   | Overlap of n-grams                           | Machine Translation |
| **ROUGE**                  | Recall of n-grams                            | Summarization       |
| **METEOR**                 | Semantic match + synonyms                    | Translation         |
| **Perplexity**             | Measures confidence (lower is better)        | Language modeling   |
| **BERTScore**              | Uses BERT embeddings for semantic similarity | Generation tasks    |
| **Exact Match / F1**       | Accuracy + overlap                           | QA tasks            |
| **Toxicity / Bias Scores** | Safety evaluation                            | Safety auditing     |

#### **Human Evaluation**:

* **Fluency**: Is it readable and grammatical?
* **Factuality**: Is the content correct?
* **Relevance**: Is the response on-topic?
* **Coherence**: Does it flow logically?
* **Helpfulness**: Does it solve the task?
* **Hallucination rate**: % of responses that are factually wrong

Automatic scores are fast but **don‚Äôt always reflect human judgment** ‚Äî human eval is essential for nuanced tasks.

---

### **31. What are hallucinations in LLMs and how can they be mitigated?**

#### **Hallucination** = When an LLM **confidently generates incorrect or fabricated information.**

* Example: ‚ÄúThe capital of Canada is Toronto.‚Äù (Wrong, it‚Äôs Ottawa.)
* Types:

  * **Factual Hallucinations**: Inventing facts
  * **Logical Hallucinations**: Contradictions or faulty reasoning
  * **Citation Hallucinations**: Fake sources, made-up links

#### **Causes**:

* Predictive nature of LLMs (they complete text plausibly, not always factually)
* Gaps or biases in training data
* Lack of real-time or grounded knowledge

#### **Mitigation Techniques**:

1. **Retrieval-Augmented Generation (RAG)**:

   * LLM retrieves real documents before answering
   * Reduces factual errors

2. **Tool Use (e.g., calculator, database)**:

   * Offloads logic and computation to reliable tools

3. **Fact-checking modules**:

   * Run post-processing checks on outputs

4. **Human-in-the-loop**:

   * Use human reviews for high-risk tasks

5. **Fine-tuning on factuality-sensitive datasets**

6. **System prompts**:

   * Remind the model to be accurate or cite sources

---

## ‚öíÔ∏è **Section 5: LLM Optimization & Tuning**

### **32. What is quantization in LLMs? What is double quantization?**

#### üîπ **Quantization**

Quantization is a technique to **reduce the memory footprint and speed up inference** of large language models by representing weights with **fewer bits**.

Instead of 32-bit floats (`fp32`), you convert weights to:

* `fp16` (half precision)
* `int8` (8-bit integers)
* `int4` or even `int2` (ultra-compressed)

This reduces:

* **Memory usage** (smaller models)
* **Inference time**
* **Power consumption**

‚ö†Ô∏è It can cause **accuracy loss** if not done carefully (especially for smaller models or sensitive tasks).

#### üîπ **Double Quantization**

Introduced in **QLoRA**, double quantization is a clever trick:

* Quantize **the quantization constants** themselves to save even more memory.
* For example, quantize 4-bit weights using 8-bit quantization of their **scaling factors**.

**Benefit:** Huge memory savings with **minimal quality drop**. Enables training 65B+ models on single GPUs (with offloading + quantization).

---

### **33. What is sharding and why is it used in large models?**

#### üîπ **Sharding** is the practice of **splitting a model's parameters or activations** across multiple devices (e.g., GPUs) or nodes (machines).

Used when a model:

* Is **too large** to fit on a single GPU (e.g., 175B parameters)
* Needs **parallel training or inference**

#### Types of Sharding:

| Type                     | Description                                                   |
| ------------------------ | ------------------------------------------------------------- |
| **Tensor Sharding**      | Splits individual tensors (e.g., layers) across GPUs          |
| **Layer Sharding**       | Assigns whole layers to different GPUs                        |
| **Pipeline Parallelism** | Different stages of computation on different GPUs             |
| **ZeRO-3 (DeepSpeed)**   | Splits optimizer state, gradients, and parameters across GPUs |

#### ‚úÖ Benefits:

* Enables **training/inference on massive models**
* Improves **scalability**
* Reduces **memory bottlenecks**

---

### **34. Explain LoRA and QLoRA. How do they reduce compute time? What do parameters like `r`, `alpha`, and `target modules` mean?**

#### üîπ **LoRA (Low-Rank Adaptation)**

LoRA fine-tunes LLMs **without changing most weights**. Instead, it adds **small low-rank matrices (ŒîW)** into the model:

```
W ‚âà W‚ÇÄ + A¬∑B   where A ‚àà ‚Ñù^{d√ór}, B ‚àà ‚Ñù^{r√ók}, r ‚â™ d, k
```

* Only **A and B** are trained (few million params vs billions).
* Original model weights remain frozen.
* Saves compute, memory, and storage.

#### üîπ **QLoRA**

Combines:

* **Quantized base model (4-bit or 8-bit)** ‚Üí saves GPU memory
* **LoRA adapters** ‚Üí add low-rank learnable layers

**Together**, QLoRA allows **efficient fine-tuning** of large quantized models even on consumer GPUs.

#### üîπ LoRA Parameters:

* `r`: **Rank** of the low-rank matrices (controls LoRA‚Äôs capacity)

  * Low `r` ‚Üí faster, less expressive
  * Common values: 4, 8, 16
* `alpha`: **Scaling factor** applied to the LoRA update (acts like learning rate)

  * Effective weight update = (Œ±/r) √ó (A¬∑B)
* `target_modules`: The layers where LoRA is applied (e.g., `q_proj`, `v_proj`, `ffn` in transformer blocks)

  * Picking the right layers is key for effective fine-tuning



During fine-tuning, LoRA (Low-Rank Adaptation) freezes the original model weights and introduces small trainable low-rank matrices into specific linear layers ‚Äî typically the query (Wq) and value (Wv) projection layers of the attention mechanism.

These low-rank matrices are used to model the weight updates, enabling the model to learn task-specific behavior without modifying the original pre-trained weights. As a result, only a few million parameters are trained, which is a small fraction compared to the billions of parameters in the full model. This makes fine-tuning much more efficient in terms of memory and compute.
---

### **35. What is PEFT (Parameter-Efficient Fine-Tuning)? How does it compare to TRL (Transformers Reinforcement Learning Library)?**

#### üîπ **PEFT (Parameter-Efficient Fine-Tuning)**

PEFT is a **category** of fine-tuning methods that **minimize the number of trainable parameters**.

It includes:

* **LoRA**
* **Adapter layers**
* **Prefix tuning**
* **IA¬≥ (Input/Output-aware Adaptation)**

üß† **Goal**: Make fine-tuning of massive models feasible on smaller hardware.

Libraries:

* Hugging Face‚Äôs [peft library](https://github.com/huggingface/peft) supports plug-and-play PEFT strategies.

#### üîπ **TRL (Transformers Reinforcement Learning)**

* A Hugging Face library focused on **fine-tuning LLMs with reinforcement learning**, especially:

  * **RLHF** (Reinforcement Learning from Human Feedback)
  * **DPO** (Direct Preference Optimization)
  * **PPO** (Proximal Policy Optimization)

üß† **Goal**: Align models with human preferences by **rewarding good outputs** and penalizing bad ones.

#### ‚úÖ **Comparison:**

| Feature | **PEFT**                     | **TRL**                                  |
| ------- | ---------------------------- | ---------------------------------------- |
| Focus   | Efficient fine-tuning        | Alignment with feedback/rewards          |
| Methods | LoRA, adapters, etc.         | PPO, DPO, RLHF                           |
| Usage   | Training with few parameters | Aligning behavior or reward optimization |
| Library | `peft`                       | `trl`                                    |

---

### **36. Explain SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning from Human Feedback).**

#### üîπ **SFT (Supervised Fine-Tuning)**

* Traditional fine-tuning using **labeled input-output pairs**
* Example:

  ```
  Prompt: "Translate to French: I love you"
  Target: "Je t‚Äôaime"
  ```
* Trains using **cross-entropy loss** to minimize error between prediction and ground truth

‚úÖ **Used in** early training stages to give model foundational task understanding.

#### üîπ **RLHF (Reinforcement Learning from Human Feedback)**

* A multi-step process to **align models with human preferences**

**Steps:**

1. **SFT Phase**: Pre-train with supervised data.
2. **Reward Modeling**: Collect multiple outputs, have humans rank them ‚Üí train a reward model.
3. **Reinforcement Learning**: Fine-tune the base model using PPO (or DPO) to maximize reward model score.

**Why?**

* SFT teaches the task
* RLHF teaches *how to be helpful, harmless, and honest*

‚úÖ **Used in** ChatGPT, Claude, and other assistant models.

---

## üìö **Section 6: Retrieval & Augmented Generation**

### **37. What is Retrieval-Augmented Generation (RAG) and how does it work?**

**RAG** is a technique that **combines LLMs with external knowledge retrieval** to improve factual accuracy, reduce hallucinations, and allow updates without retraining.

#### üîπ **How RAG Works:**

1. **Query Encoding**:

   * Convert user input into a vector (embedding) using a transformer (e.g., Sentence-BERT).
2. **Document Retrieval**:

   * Use that vector to **search a vector database** for the most relevant documents (chunks, passages).
3. **Augmented Prompting**:

   * Inject retrieved documents into the **context window** of the LLM.
4. **Generation**:

   * The LLM generates an answer **conditioned on both the prompt and the retrieved context**.

#### üß† **Why use RAG?**

* LLMs are **frozen** after training; they can‚Äôt learn new facts post-cutoff.
* RAG gives the model **access to up-to-date or private data** without retraining.
* Reduces **hallucinations** by grounding responses in real content.

#### üîÅ RAG = Retrieve ‚Üí Read ‚Üí Respond


**Retrieval-Augmented Generation (RAG)** is a technique that combines **large language models (LLMs)** with an **external knowledge base** to reduce hallucinations and incorporate up-to-date or domain-specific information ‚Äî **without retraining the model**.
In RAG, relevant documents (such as pages, PDFs, or text chunks) are stored in a **vector database**, where each document is converted into an **embedding** ‚Äî a dense vector that captures its meaning.
During inference, the input query is also converted into an embedding using the same model. This embedding is used to **search the vector database for the most semantically similar documents**.
The retrieved documents (also called **retrieval context**) are then used to **augment the original prompt**. This **enriched prompt** is passed to the LLM, which uses both the query and the retrieved knowledge to **generate a more accurate, informed response**.

This process allows the model to access information **outside its training data** and dynamically use it at runtime ‚Äî which is especially useful for domains with **frequent updates** (like law, medicine, or finance) or **custom organizational data**.

---

### **38. Compare RAG vs. Fine-Tuning: Which to use and when?**

| Criteria            | **RAG**                                              | **Fine-Tuning**                                     |
| ------------------- | ---------------------------------------------------- | --------------------------------------------------- |
| **Cost**            | Cheaper (no model retraining)                        | Expensive (requires compute + storage)              |
| **Flexibility**     | Easily update content by modifying documents         | Updating requires retraining                        |
| **Use-case fit**    | Dynamic, evolving content (e.g., FAQs, company docs) | Stable tasks or formats (e.g., legal summarization) |
| **Data type**       | Works well with **text knowledge bases**             | Works well with labeled **input-output pairs**      |
| **Latency**         | Slightly higher (retrieval step)                     | Often faster at inference time                      |
| **Custom behavior** | Less control over tone/style                         | High control (e.g., model sounds like your brand)   |
| **Example**         | Chatbot answering questions about internal docs      | Tuning GPT to follow legal writing conventions      |

‚úÖ **Use RAG** when:

* You want the model to stay **up-to-date** with dynamic or proprietary data
* You don‚Äôt have labeled data
* You want **factual grounding**

‚úÖ **Use Fine-Tuning** when:

* You have **high-quality labeled datasets**
* You need custom **style, tone, or structured outputs**
* You're okay with **freezing content** inside the model

Often, **RAG + fine-tuning** is combined: fine-tune the model on your tone, and use RAG for knowledge injection.

---

### **39. What is a vector database? How is it different from a traditional database? Give some examples.**

#### üîπ **Vector Database**

A **vector database** stores and indexes **high-dimensional vectors** (embeddings). It‚Äôs optimized for **similarity search** using metrics like **cosine similarity** or **Euclidean distance**.

Used for:

* Semantic search
* RAG (Retrieval-Augmented Generation)
* Recommendation systems
* Image/audio similarity

#### üîπ Key Differences from Traditional DBs:

| Feature       | **Traditional DB (SQL/NoSQL)**                               | **Vector DB**                                                  |
| ------------- | ------------------------------------------------------------ | -------------------------------------------------------------- |
| **Data Type** | Structured (tables, rows)                                    | Unstructured (vectors, embeddings)                             |
| **Query**     | Exact match or filters (e.g., `SELECT * WHERE name="Alice"`) | Similarity-based (`find top-5 vectors nearest to this vector`) |
| **Indexing**  | B-trees, hash tables                                         | FAISS, HNSW (graph-based), IVF                                 |
| **Use Case**  | CRUD apps, financial data, logs                              | Semantic search, RAG, similarity search                        |

#### üîπ Examples of Vector DBs:

| Tool         | Notes                                                                    |
| ------------ | ------------------------------------------------------------------------ |
| **FAISS**    | Facebook AI's library for fast approximate similarity search (in-memory) |
| **Pinecone** | Managed vector DB, scalable and production-ready                         |
| **Weaviate** | Open-source + semantic search with filters                               |
| **ChromaDB** | Lightweight, often used with LangChain                                   |
| **Milvus**   | Highly scalable open-source alternative                                  |
| **Qdrant**   | Fast, Rust-based, good for real-time use                                 |

---

Would you like a code example showing how to build a RAG pipeline with a vector database (e.g., using LangChain + FAISS)?




----

## Transformer architecture

 The **Transformer architecture** has two main components: the **encoder** and the **decoder**.

 #### Encoder:

 The encoder begins with an **embedding layer**, which converts input words or tokens into dense **embedding vectors**. After that, **positional encoding** is added to the embeddings to provide information about the position of each token in the sequence, since transformers do not have built-in notion of order like RNNs.

 The next key component is the **self-attention mechanism**. This allows each token to **attend to every other token in the input**, including itself. This is crucial because it helps the model understand relationships and dependencies between words, regardless of their position in the sequence.

 Transformers use **multi-head attention**, which means the self-attention mechanism is applied in parallel multiple times with different parameter sets. This allows the model to capture different types of relationships in different subspaces.

 The output of the attention mechanism is then **added back to the original input of the attention layer** using a **residual connection**, and **layer normalization** is applied (you missed this part). This output is then passed through a **feed-forward neural network** (which is applied position-wise), followed again by residual connection and layer normalization.

 The encoder typically has multiple such identical blocks stacked on top of each other.

 #### Decoder:

 The decoder also starts with an **embedding layer** and **positional encoding**, just like the encoder. However, its attention layers are slightly different.

 First, it applies **masked self-attention**, which ensures that each position can only attend to **previous positions** (not future ones). This is important during generation so that the model doesn‚Äôt ‚Äúpeek ahead‚Äù.

 Then, there is a second attention layer called **encoder-decoder attention**, where the decoder attends to the **output of the encoder**. This lets the decoder use information from the input sequence to help predict the output.

 The output of this process is again passed through a **feed-forward layer**, along with residual connections and normalization.

 The decoder produces one token at a time. During generation, it starts with a special start token (`<sos>`) and tries to predict the next token. That predicted token is then fed back into the decoder to predict the next one. This process continues until a special **end-of-sequence token** (`<eos>`) is generated.



---

LORA

